# 机器学习模型可解释性

> https://mp.weixin.qq.com/s/JEIxzuPDrbvSJjpHExaI_w

机器学习流程步骤：收集数据、清洗数据、训练模型、基于验证或测试错误或其他评价指标选择最好的模型。

选取模型时, 我们往往考虑准确率与模型复杂度, 我们希望选择比较小的错误率和比较高的准确率的高精度的模型, 同时选择一个相对复杂的模型(?)。

面临准确率和模型复杂度之间的权衡，但一个模型越复杂就越难以解释.

深度神经网络处于一个极端，因为它们能够在多个层次进行抽象推断，所以他们可以处理因变量与自变量之间非常复杂的关系，并且达到非常高的精度。但是这种复杂性也使模型成为黑箱，我们无法获知所有产生模型预测结果的这些特征之间的关系，所以我们只能用准确率、错误率这样的评价标准来代替，来评估模型的可信性。



事实上，每个分类问题的机器学习流程中都应该包括模型理解和模型解释，下面是几个原因：

- **模型改进**：理解指标特征、分类、预测，进而理解为什么一个机器学习模型会做出这样的决定、**什么特征在决定中起最重要作用**，能让我们判断模型是否符合常理。一个深度的神经网络来学习区分狼和哈士奇的图像。模型使用大量图像训练，并使用另外的一些图像进行测试。90%的图像被准确预测，这值得我们高兴。但是在没有计算解释函数(explainer function)时，我们不知道该模型主要基于背景：狼图像通常有一个下雪的背景，而哈士奇的图像很少有。所以我们不知不觉地做了一个雪地探测器，如果只看准确率这样的指标，我们就不会看到这一点。知道了模型是如何使用特征进行预测的，我们就能直觉地判断我们的模型是否抓住了有意义的特征，模型是或否能泛化到其他样本的预测上。(有意义的特征)
- **模型可信性与透明度**：理解机器学习模型在提高模型可信度和提供审视预测结果透明度上是非常必要的，让黑箱模型来决定人们的生活是不现实的，比如贷款和监狱刑法。另一个对机器学习结果可信度提出质疑的领域是药品，模型结果会直接决定病人的生与死。机器学习模型在区分恶性肿瘤和不同类型的良性肿瘤方面是非常准确的，但是我们依然需要专家对诊断结果进行解释，解释为什么一个机器学习模型将某个患者的肿瘤归类为良性或恶性将大大帮助医生信任和使用机器学习模型来支持他们工作。长久来看，更好地理解机器学习模型可以节省大量时间、防止收入损失。如果一个模型没有做出合理的决定，在应用这个模型并造成不良影响之前，我们就可以发现这一点。(重要领域需要让黑箱透明, 进而可信)
- **识别和防止偏差**：方差和偏差是机器学习中广泛讨论的话题。有偏差的模型经常由有偏见的事实导致，如果数据包含微妙的偏差，模型就会学习下来并认为拟合很好。一个有名的例子是，用机器学习模型来为囚犯建议定罪量刑，这显然反映了司法体系在种族不平等上的内在偏差。其他例子比如用于招聘的机器学习模型，揭示了在特定职位上的性别偏差，比如男性软件工程师和女性护士。机器学习模型在我们生活的各个层面上都是强有力的工具，而且它也会变得越来越流行。所以作为数据科学家和决策制定者来说，理解我们训练和发布的模型如何做出决策，让我们可以事先预防偏差的增大以及消除他们，是我们的责任。(理解模型决策, 预防偏差增大)

可解释性特质：

- **重要性**：了解“为什么”可以帮助更深入地了解问题，数据以及模型可能失败的原因。
- **分类**：建模前数据的可解释性、建模阶段模型可解释性、运行阶段结果可解释性。
- **范围**：全局解释性、局部解释性、模型透明度、模型公平性、模型可靠性。
- **评估**：内在还是事后？模型特定或模型不可知？本地还是全局？
- **特性**：准确性、保真性、可用性、可靠性，鲁棒性、通用性等。
- **人性化解释**：人类能够理解决策原因的程度，人们可以持续预测模型结果的程度标示。

### 理解模型可解释性

模型解释作为一个概念仍然主要是理论和主观的。任何机器学习模型的核心都有一个**响应函数**，它试图**映射**和解释**独立（输入）自变量**和**（目标或响应）因变量**之间的**关系和模式**。当模型预测或寻找见解时，需要做出某些决定和选择。模型解释试图理解和解释响应函数所做出的这些决定，即what，why以及how。模型解释的关键是透明度，质疑能力以及人类理解模型决策的难易程度。模型解释的三个最重要的方面解释如下。

1. **是什么驱动了模型的预测？**我们应该能够查询我们的模型并找出潜在的特征交互，以了解哪些特征在模型的决策策略中可能是重要的。这确保了模型的公平性。
2. **为什么模型会做出某个决定？**我们还应该能够验证并证明为什么某些关键特征在预测期间驱动模型所做出的某些决策时负有责任。这确保了模型的可靠性。
3. **我们如何信任模型预测？**我们应该能够评估和验证任何数据点以及模型如何对其进行决策。对于模型按预期工作的关键利益相关者而言，这应该是可证明且易于理解的。这确保了模型的透明度。

在比较模型时，除了模型性能之外，如果模型的决策比其他模型的决策更容易理解，那么模型被认为比其他模型具有更好的可解释性。

### 可解释性的重要性

如果一个模型工作得很好，为什么还要深入挖掘呢？在解决现实世界中的数据科学问题时，为了让企业信任您的模型预测和决策，他们会不断提出“我为什么要相信您的模型？”这一问题，这一点非常有意义。如果一个人患有癌症或糖尿病，一个人可能对社会构成风险，或者即使客户会流失，您是否会对预测和做出决策（如果有的话）感到满意？也许不是，如果我们能够更多地了解模型的决策过程（原因和方式），我们可能会更喜欢它。这使我们更加透明地了解模型为何做出某些决策，在某些情况下可能出现的问题，并且随着时间的推移它有助于我们在这些机器学习模型上建立一定程度的信任。

- 了解预测背后的原因在评估信任方面非常重要，如果计划基于预测采取行动，或者选择是否部署新模型，那么这是至关重要的。
- 无论人类是直接使用机器学习分类器作为工具，还是在其他产品中部署模型，仍然存在一个至关重要的问题：如果用户不信任模型或预测，他们就不会使用它。