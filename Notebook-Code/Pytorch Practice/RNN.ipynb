{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN，并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。\n",
    "\n",
    "这里通过多层感知机，来推演如何通过添加隐藏状态来将它变成循环神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设输入样本数为n，输入特征数为d，则$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$，设激活函数为$\\phi$，那么隐藏层的输出$\\boldsymbol{H} \\in \\mathbb{R}^{n \\times h}$计算为：\n",
    "$$\n",
    "\\boldsymbol{H}=\\phi\\left(\\boldsymbol{X} \\boldsymbol{W}_{x h}+\\boldsymbol{b}_{h}\\right)\n",
    "$$\n",
    "输出层输出为：\n",
    "$$\n",
    "\\boldsymbol{O}=\\boldsymbol{H} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}\n",
    "$$\n",
    "其中，输出变量$\\boldsymbol{O} \\in \\mathbb{R}^{n \\times q}$\n",
    "\n",
    "# 含隐藏状态的RNN\n",
    "假设$\\boldsymbol{X}_{t} \\in \\mathbb{R}^{n \\times d}$是序列中的某步输入，$\\boldsymbol{H}_{t} \\in \\mathbb{R}^{n \\times h}$是该步的隐藏变量。\n",
    "\n",
    "**与多层感知机不同，这里保存上一时间步的隐藏变量$H_{t-1}$，并引入一个新的权重参数**$\\boldsymbol{W}_{h h} \\in \\mathbb{R}^{h \\times h}$，该参数描述当前时间如何使用上一时间步的隐藏变量：\n",
    "$$\n",
    "\\boldsymbol{H}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right)\n",
    "$$\n",
    "我们的RNN代码都基于上式计算，输出层计算则为：\n",
    "$$\n",
    "\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}\n",
    "$$\n",
    "故RNN中包含的隐藏层权重参数为：$\\boldsymbol{W}_{x h} \\in \\mathbb{R}^{d \\times h}, \\quad \\boldsymbol{W}_{h h} \\in \\mathbb{R}^{h \\times h}$和bias$\\boldsymbol{b}_{h} \\in \\mathbb{R}^{1 \\times h}$\n",
    "\n",
    "RNN输出层包含参数：$\\boldsymbol{W}_{h q} \\in \\mathbb{R}^{h \\times q}$和$\\boldsymbol{b}_{q} \\in \\mathbb{R}^{1 \\times q}$\n",
    "\n",
    "即使在不同时间步，RNN也始终使用这些模型参数，因此，**RNN模型参数的数量不随时间步的增加而增长**。\n",
    "\n",
    "$\\boldsymbol{H}_{t}=\\phi(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h h})$\n",
    "等价于将$\\boldsymbol{X}_{t}$和$\\boldsymbol{H}_{t-1}$连结后的矩阵乘以$\\boldsymbol{W}_{x h}$与$\\boldsymbol{W}_{h h}$连结后的矩阵（但是注意两者的连结方式不同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4125,  1.5532, -0.7379,  0.5852],\n",
       "        [-0.0409,  1.2636,  1.3743, -1.0091],\n",
       "        [ 1.6653,  1.0242, -1.1717,  1.3498]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 不采用连结的方式\n",
    "X, W_xh = torch.randn(3, 1), torch.randn(1, 4)\n",
    "H, W_hh = torch.randn(3, 4), torch.randn(4, 4)\n",
    "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4125,  1.5532, -0.7379,  0.5852],\n",
       "        [-0.0409,  1.2636,  1.3743, -1.0091],\n",
       "        [ 1.6653,  1.0242, -1.1717,  1.3498]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X H -> (3, 5)\n",
    "torch.matmul(torch.cat((X, H), dim=1), torch.cat((W_xh, W_hh), dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于字符级循环神经网络的语言模型\n",
    "\n",
    "因为每个输入词是一个字符，因此这个模型被称为字符级RNN。因为不同字符的个数远小于不同词的个数，所以字符级RNN的计算通常更加简单。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
