{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化空Tensor\n",
    "torch.empty(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0967, 0.1758, 0.5492, 0.6219],\n",
       "        [0.5799, 0.2373, 0.3336, 0.3543],\n",
       "        [0.0855, 0.3944, 0.6154, 0.0253],\n",
       "        [0.4470, 0.4222, 0.8033, 0.8204],\n",
       "        [0.5248, 0.6441, 0.1726, 0.3691]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化随机Tensor\n",
    "torch.rand(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建指定类型的Tensor\n",
    "torch.zeros(5,4,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接根据数据创建\n",
    "torch.tensor([5.5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-1.6173,  0.9602,  1.2484,  0.5259],\n",
      "        [ 0.7985, -3.2632,  0.0292,  1.0837],\n",
      "        [-0.7441,  0.3033, -0.8035, -0.0135],\n",
      "        [-0.2141,  0.2090, -0.4113, -0.3663],\n",
      "        [ 0.2824,  0.3437,  0.1070, -0.4857]])\n"
     ]
    }
   ],
   "source": [
    "# 通过现有的tensor创建\n",
    "x = torch.tensor([5.5, 3])\n",
    "x = x.new_ones(5, 4, dtype=torch.float64)\n",
    "print(x)\n",
    "# 指定鑫的数据类型\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# 获取tensor大小, 返回的是一个tuple\n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([1, 3, 5, 7])\n",
      "tensor([1., 5., 9.])\n",
      "tensor([[0.3685, 0.1121, 0.4180, 0.0526, 0.2030],\n",
      "        [0.7330, 0.0997, 0.2582, 0.5303, 0.5868],\n",
      "        [0.5660, 0.1070, 0.1867, 0.0315, 0.7060],\n",
      "        [0.4836, 0.3948, 0.1635, 0.0540, 0.1190],\n",
      "        [0.5635, 0.8470, 0.3006, 0.2175, 0.0794]], dtype=torch.float64)\n",
      "tensor([[-0.1352, -1.8878,  1.5413, -0.0860, -0.9869],\n",
      "        [-0.6362, -0.4584, -1.0627, -0.0824,  0.5564],\n",
      "        [-0.4171, -0.3533,  0.2015, -0.6528, -0.8556],\n",
      "        [-0.0900,  1.9175, -0.8044, -1.4511, -0.1933],\n",
      "        [ 0.6794,  1.0957, -0.6286, -1.2469,  0.8569]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8781, 2.4356, 2.9232, 2.4095, 5.2940, 6.0918, 7.0069, 7.9226, 8.7711,\n",
       "        9.8844], dtype=torch.float64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.eye(3, 3))\n",
    "print(torch.arange(1,9, step=2))\n",
    "print(torch.linspace(1,9,steps=3))\n",
    "# 均匀分布\n",
    "print(torch.rand(5,5, dtype=torch.float64))\n",
    "# 标准分布\n",
    "print(torch.randn(5,5, dtype=torch.float64))\n",
    "# 正态分布 每个mean和std--标准差返回一个张量, 张量里面的随机数是从相互独立的正态分布中随机生成的\n",
    "torch.normal(mean=torch.arange(1, 11, dtype=torch.float64),std=torch.arange(1, 0, -0.1, dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算术操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9635,  1.5426,  1.2975,  1.2901],\n",
       "        [ 0.9653, -2.7907,  0.7404,  2.0365],\n",
       "        [-0.5530,  0.8305, -0.6107,  0.4314],\n",
       "        [-0.1432,  0.2725,  0.2560, -0.2269],\n",
       "        [ 1.2329,  0.9221,  0.9120, -0.4337]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,4)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9635,  1.5426,  1.2975,  1.2901],\n",
       "        [ 0.9653, -2.7907,  0.7404,  2.0365],\n",
       "        [-0.5530,  0.8305, -0.6107,  0.4314],\n",
       "        [-0.1432,  0.2725,  0.2560, -0.2269],\n",
       "        [ 1.2329,  0.9221,  0.9120, -0.4337]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9635,  1.5426,  1.2975,  1.2901],\n",
       "        [ 0.9653, -2.7907,  0.7404,  2.0365],\n",
       "        [-0.5530,  0.8305, -0.6107,  0.4314],\n",
       "        [-0.1432,  0.2725,  0.2560, -0.2269],\n",
       "        [ 1.2329,  0.9221,  0.9120, -0.4337]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.rand(5,4)\n",
    "# 指定输出给result\n",
    "torch.add(x,y,out=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6538, 0.5824, 0.0491, 0.7642],\n",
      "        [0.1668, 0.4725, 0.7113, 0.9528],\n",
      "        [0.1911, 0.5272, 0.1928, 0.4449],\n",
      "        [0.0709, 0.0635, 0.6673, 0.1394],\n",
      "        [0.9505, 0.5784, 0.8050, 0.0520]])\n",
      "tensor([[-0.9635,  1.5426,  1.2975,  1.2901],\n",
      "        [ 0.9653, -2.7907,  0.7404,  2.0365],\n",
      "        [-0.5530,  0.8305, -0.6107,  0.4314],\n",
      "        [-0.1432,  0.2725,  0.2560, -0.2269],\n",
      "        [ 1.2329,  0.9221,  0.9120, -0.4337]])\n"
     ]
    }
   ],
   "source": [
    "# add x -> y, 称作 inplace\n",
    "print(y)\n",
    "print(y.add_(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 矩阵变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5426],\n",
       "        [-2.7907],\n",
       "        [ 0.8305],\n",
       "        [ 0.2725],\n",
       "        [ 0.9221]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在指定维度dim上选取，⽐比如选取某些⾏行行、某些列列\n",
    "torch.index_select(y, dim=1, index=torch.arange(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4]) torch.Size([20]) torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# view改变tensor形状 这种方式下x y z 是共享内存的\n",
    "y = x.view(20)\n",
    "z = x.view(-1, 5) # -1所指的维度可以根据其他维度的值推出来 \n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回一个新副本\n",
    "x_cp = x.clone().view(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7705])\n",
      "0.7705495357513428\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1)\n",
    "print(x)\n",
    "# 转为python number\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4976,  0.3496, -0.1921, -0.4762,  1.9057],\n",
       "        [ 0.8230,  1.0034,  1.9424, -0.6997,  2.0571],\n",
       "        [ 0.4945, -1.5219, -0.1745, -1.0431, -0.1724],\n",
       "        [ 1.0605, -1.2459,  0.2497, -0.2616,  0.2946],\n",
       "        [-0.0865,  1.1064, -0.1524, -0.2232,  0.2405]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnX = torch.randn(5,5)\n",
    "nnX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3102)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线元素和\n",
    "torch.trace(nnX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4976,  1.0034, -0.1745, -0.2616,  0.2405])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线元素\n",
    "torch.diag(nnX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4976,  0.3496, -0.1921, -0.4762,  1.9057],\n",
      "        [ 0.0000,  1.0034,  1.9424, -0.6997,  2.0571],\n",
      "        [ 0.0000,  0.0000, -0.1745, -1.0431, -0.1724],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.2616,  0.2946],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2405]]) \n",
      " tensor([[-0.4976,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8230,  1.0034,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4945, -1.5219, -0.1745,  0.0000,  0.0000],\n",
      "        [ 1.0605, -1.2459,  0.2497, -0.2616,  0.0000],\n",
      "        [-0.0865,  1.1064, -0.1524, -0.2232,  0.2405]])\n"
     ]
    }
   ],
   "source": [
    "# 上/下三角\n",
    "print(torch.triu(nnX), '\\n',torch.tril(nnX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4976],\n",
       "          [ 0.3496],\n",
       "          [-0.1921],\n",
       "          [-0.4762],\n",
       "          [ 1.9057]],\n",
       " \n",
       "         [[ 0.8230],\n",
       "          [ 1.0034],\n",
       "          [ 1.9424],\n",
       "          [-0.6997],\n",
       "          [ 2.0571]],\n",
       " \n",
       "         [[ 0.4945],\n",
       "          [-1.5219],\n",
       "          [-0.1745],\n",
       "          [-1.0431],\n",
       "          [-0.1724]],\n",
       " \n",
       "         [[ 1.0605],\n",
       "          [-1.2459],\n",
       "          [ 0.2497],\n",
       "          [-0.2616],\n",
       "          [ 0.2946]]]), tensor([[-0.8836,  0.2607,  0.3200],\n",
       "         [ 0.3016, -0.7118, -1.1581],\n",
       "         [ 0.2088, -0.1249, -0.3597],\n",
       "         [-0.2403, -0.2901,  0.2573],\n",
       "         [-0.2214, -1.3419, -1.4137]]), tensor([[[-0.5830]],\n",
       " \n",
       "         [[-0.9038]],\n",
       " \n",
       "         [[-1.4070]],\n",
       " \n",
       "         [[ 0.0577]]]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnY_2 = torch.randn(5,3)\n",
    "nnY_1 = torch.randn(15,1,1)[:4,:,:]\n",
    "nnX_3 = nnX.clone().view(5,5,1)[:4,:,:]\n",
    "nnX_3, nnY_2,nnY_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1974, -2.7738, -3.3115],\n",
      "        [-0.3063, -3.2997, -4.6855],\n",
      "        [-0.6436,  1.7680,  1.9589],\n",
      "        [-1.2630,  0.8128,  1.2086],\n",
      "        [ 0.3787, -1.0490, -1.6515]]) \n",
      " tensor([[[ 0.2901],\n",
      "         [-0.2038],\n",
      "         [ 0.1120],\n",
      "         [ 0.2776],\n",
      "         [-1.1111]],\n",
      "\n",
      "        [[-0.7438],\n",
      "         [-0.9069],\n",
      "         [-1.7555],\n",
      "         [ 0.6324],\n",
      "         [-1.8591]],\n",
      "\n",
      "        [[-0.6957],\n",
      "         [ 2.1412],\n",
      "         [ 0.2455],\n",
      "         [ 1.4676],\n",
      "         [ 0.2426]],\n",
      "\n",
      "        [[ 0.0612],\n",
      "         [-0.0719],\n",
      "         [ 0.0144],\n",
      "         [-0.0151],\n",
      "         [ 0.0170]]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "# 虽然我还不懂bmm是什么意思, 不过两个输入, 前后都得是3 dim, 后面一个只有一个维度值, 且维度要与前一个参数的个数一致\n",
    "print(torch.mm(nnX,nnY_2),'\\n',torch.bmm(nnX_3,nnY_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.4092)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnX_1 = nnX.view(25)\n",
    "# 内积, cross 外积\n",
    "nnX_1.dot(nnX_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.svd(\n",
       "U=tensor([[ 0.4333,  0.0447, -0.8730,  0.2171,  0.0316],\n",
       "        [ 0.8696, -0.2117,  0.3902, -0.0948, -0.1944],\n",
       "        [-0.1645, -0.6677, -0.2784, -0.6147, -0.2677],\n",
       "        [-0.0024, -0.6398,  0.0815,  0.3667,  0.6705],\n",
       "        [ 0.1703,  0.3131, -0.0388, -0.6569,  0.6633]]),\n",
       "S=tensor([3.5437, 2.5693, 1.5650, 0.7636, 0.4663]),\n",
       "V=tensor([[ 0.1133, -0.4796,  0.4521, -0.0581,  0.7412],\n",
       "        [ 0.4136,  0.7640,  0.2336, -0.3500,  0.2612],\n",
       "        [ 0.4538, -0.1988,  0.6392,  0.0958, -0.5804],\n",
       "        [-0.1921,  0.3584,  0.2687,  0.8575,  0.1646],\n",
       "        [ 0.7571, -0.1356, -0.5102,  0.3600,  0.1360]]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 奇异值分解 nnX.svd() 也可以\n",
    "torch.svd(nnX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2130, -0.1218, -0.3398,  1.1809,  1.0401],\n",
       "        [-0.1483,  0.0313, -0.1275,  0.0292,  0.7798],\n",
       "        [-0.3166,  0.5171,  0.1729, -0.7061, -0.9263],\n",
       "        [ 0.0879, -0.1847, -0.9169,  0.5733, -0.4758],\n",
       "        [ 0.4864, -0.0316, -0.2770,  0.3751, -0.0837]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求逆\n",
    "nnX.inverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广播机制\n",
    "不同形状Tensor计算时, 可能会触发广播, 一般先适当复制一下两个Tensor使其形状相同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]]) \n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1,3).view(1,2)\n",
    "y = torch.arange(1,4).view(3,1)\n",
    "print(x,'\\n',y)\n",
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算内存开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y False\n",
      "y[:] True\n"
     ]
    }
   ],
   "source": [
    "# 用id()函数获取内存地址\n",
    "x = torch.Tensor([1,2])\n",
    "y = torch.Tensor([3,4])\n",
    "id_before = id(y)\n",
    "y = x + y\n",
    "print('y', id(y) == id_before)\n",
    "id_before = id(y)\n",
    "# 同样可以使用 torch.add(x, y, out=y) 或者 y.add_(x) 达到与下面一致的效果\n",
    "y[:] = x + y\n",
    "print('y[:]', id(y) == id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 和 NUMPY 的相互转换\n",
    "numpy() from_numpy(), 这两个函数产生的Tensor与Numpy都会共享内存\n",
    "torch.tensor()将numpy转换成tensor, 该方法是数据拷贝, 不会共享内存.\n",
    "除了CharTensor, 都支持转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求梯度\n",
    "pytorch 提供的 autograd 能够根据输入和前向传播自动构建的计算图, 执行反向传播.\n",
    "\n",
    "将tensor的属性requires_grad=True, 将开始追踪其上所有的操作, 这样就可以使用backward()方法求梯度了, 梯度将累积到tensor.grad中.\n",
    "\n",
    "不想被追踪的话, 用.detach()方法取消就行了.\n",
    "\n",
    "## Function\n",
    "Tensor和Function互相结合就可以构建一个记录整个计算过程的有向无环图(DAG)\n",
    "\n",
    "每个Tensor都有个.grad_fn属性, 该属性创建这个Tensor的Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x103d2a9d0>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x是直接创建的, 相当于一个常数, 所以没有grad_fn, y是通过**加法**操作创建的, 所以他有一个为AddBackward的grad_fn\n",
    "\n",
    "x这种节点称为叶子节点(因为grad_fn为None)\n",
    "\n",
    "不允许张量(高维矩阵)求道, 只允许标量对张量求导, 求导结果是和自变量(标量)同形的张量.\n",
    "(不允许 Tensor 对 Tensor 求导，只允许标量 Scalar 对张量 Tensor 求导，求导结果是和自变量同型的 Tensor。)\n",
    "\n",
    "举个例⼦，假设y由自变量x计算⽽而来，w是和y同形的张量，则y.backward(w)的含义是:先计算l = torch.sum(y * w)，则l是个标量，然后求l对自变量x的导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x.is_leaf, y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) \n",
      " tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, '\\n', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "o=\\frac{1}{4} \\sum_{i=1}^{4} z_{i}=\\frac{1}{4} \\sum_{i=1}^{4} 3\\left(x_{i}+2\\right)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19.5393,  1.6918],\n",
      "        [ 0.5934,  1.5469]], requires_grad=True)\n",
      "<SumBackward0 object at 0x103d21590>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "# 通过requires_grad_函数改变属性\n",
    "a.requires_grad_(True)\n",
    "print(a)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度\n",
    "前面的out, 是一个标量, 所以可以直接调用backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 看 d(out) / dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left.\\frac{\\partial o}{\\partial x_{i}}\\right|_{x_{i}=1}=\\frac{9}{2}=4.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量y关于x的梯度是一个雅可比矩阵\n",
    "$$\n",
    "J=\\left(\\begin{array}{ccc}{\\frac{\\partial y_{1}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial y_{1}}{\\partial x_{n}}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial y_{m}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial y_{m}}{\\partial x_{n}}}\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$o$是一个标量函数$l=g(y)$的梯度\n",
    "$$\n",
    "o=\\left(\\begin{array}{ccc}{\\frac{\\partial l}{\\partial y_{1}}} & {\\cdots} & {\\frac{\\partial l}{\\partial y_{m}}}\\end{array}\\right)\n",
    "$$\n",
    "那么根据链式法则$o$关于$x$的雅可比矩阵为:\n",
    "$$\n",
    "o J=\\left(\\begin{array}{ccc}{\\frac{\\partial l}{\\partial y_{1}}} & {\\cdots} & {\\frac{\\partial l}{\\partial y_{m}}}\\end{array}\\right)\\left(\\begin{array}{ccc}{\\frac{\\partial y_{1}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial y_{1}}{\\partial x_{n}}} \\\\ {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\frac{\\partial y_{m}}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial y_{m}}{\\partial x_{n}}}\\end{array}\\right)=\\left(\\begin{array}{ccc}{\\frac{\\partial l}{\\partial x_{1}}} & {\\cdots} & {\\frac{\\partial l}{\\partial x_{n}}}\\end{array}\\right)\n",
    "$$\n",
    "注意, grad反向传播过程中是累加的, 这意味着每一次反向传播, 梯度都会累加, 所以, 一般反向传播之前, 会把之前的梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 累加\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "#清零\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], dtype=torch.float64, grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float64, requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2,2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20., dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor([2., 4., 6., 8.], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# l = torch.sum(torch.tensor([1.0, 0.1,0.01, 0.001]) * y)\n",
    "l = torch.sum(torch.tensor([1.0, 1.0, 1.0, 1.0]) * y)\n",
    "print(l)\n",
    "# 只有是标量才能\n",
    "l.backward()\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200, 0.0020], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float64)\n",
    "z.backward(v)\n",
    "# x.grad是和x同维的Tensor\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1=x**2\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3=y1+y2\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad) # True\n",
    "print(y2, y2.requires_grad) # False\n",
    "print(y3, y3.requires_grad) # True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
