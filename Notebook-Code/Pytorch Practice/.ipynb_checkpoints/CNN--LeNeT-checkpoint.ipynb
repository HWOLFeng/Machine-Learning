{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 相邻像素在像素中可能举例较远，难以被模型识别\n",
    "2. 对于大尺寸图像，全连接层容易造成模型过大，权重参数过多，消耗资源较多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet模型\n",
    "\n",
    "分为卷积层块和全连接层块两个部分。\n",
    "\n",
    "## 卷积层块\n",
    "卷积层块里的基本单位是**卷积层** 后接 **池化层**，前者用来识别图像里的空间模式，后者用来降低卷积层对位置的敏感性。\n",
    "\n",
    "卷积层：5 * 5的窗口，输出接sigmoid激活函数。第一层输出通道为6，第二层输出通道数则增加到16，增加输出通道的目的是使得两层的参数尺寸类似。\n",
    "\n",
    "池化层：窗口形状为 2 * 2，步幅为2，这样池化窗口每次覆盖的区域互不重叠。\n",
    "\n",
    "卷积层块输出形状：（批量大小，通道，高，宽）\n",
    "\n",
    "\n",
    "## 全连接层块\n",
    "当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。\n",
    "\n",
    "全连接层块含3个全连接层，它们输出个数分别是120、84、10。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channel, out_channels, kernel_size \n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "        def forward(self, img):\n",
    "            feature = self.conv(img)\n",
    "            # -1 表示除了将batch_size的留住，其他都平铺\n",
    "            output = self.fc(feature.view(img.shape[0], -1))\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.2074, 0.2054, 0.5633, 0.3730, 0.8804],\n",
       "           [0.4153, 0.4657, 0.5863, 0.3955, 0.8333],\n",
       "           [0.1516, 0.9421, 0.3776, 0.0644, 0.0509],\n",
       "           [0.3256, 0.8615, 0.0672, 0.2774, 0.1863]],\n",
       " \n",
       "          [[0.8762, 0.0150, 0.1385, 0.0170, 0.7206],\n",
       "           [0.3973, 0.1492, 0.1111, 0.1172, 0.1541],\n",
       "           [0.6786, 0.7248, 0.4294, 0.8560, 0.8313],\n",
       "           [0.6508, 0.5372, 0.6621, 0.8178, 0.3615]],\n",
       " \n",
       "          [[0.0988, 0.4579, 0.3510, 0.6998, 0.2864],\n",
       "           [0.4671, 0.0352, 0.8265, 0.4562, 0.7443],\n",
       "           [0.5871, 0.8109, 0.0035, 0.3449, 0.8018],\n",
       "           [0.7328, 0.4198, 0.7526, 0.5755, 0.8206]]],\n",
       " \n",
       " \n",
       "         [[[0.9563, 0.4376, 0.7134, 0.9166, 0.5823],\n",
       "           [0.9733, 0.5761, 0.2175, 0.8851, 0.2554],\n",
       "           [0.0991, 0.2961, 0.9611, 0.6106, 0.0844],\n",
       "           [0.1208, 0.1454, 0.3462, 0.2565, 0.8868]],\n",
       " \n",
       "          [[0.0856, 0.0987, 0.3582, 0.1966, 0.4465],\n",
       "           [0.8674, 0.2877, 0.4052, 0.2205, 0.4557],\n",
       "           [0.1451, 0.9533, 0.5063, 0.4957, 0.2135],\n",
       "           [0.6175, 0.5078, 0.8189, 0.5772, 0.3823]],\n",
       " \n",
       "          [[0.4847, 0.6292, 0.5457, 0.1908, 0.3470],\n",
       "           [0.6417, 0.7658, 0.8591, 0.5521, 0.3953],\n",
       "           [0.6925, 0.5183, 0.6053, 0.6116, 0.0647],\n",
       "           [0.4607, 0.1917, 0.6527, 0.4685, 0.8534]]],\n",
       " \n",
       " \n",
       "         [[[0.3328, 0.9210, 0.2726, 0.5785, 0.2683],\n",
       "           [0.5808, 0.8809, 0.4370, 0.7120, 0.0318],\n",
       "           [0.0265, 0.6075, 0.2960, 0.7722, 0.7348],\n",
       "           [0.0707, 0.1790, 0.0715, 0.7037, 0.9577]],\n",
       " \n",
       "          [[0.9155, 0.5568, 0.0687, 0.7477, 0.5852],\n",
       "           [0.6463, 0.1502, 0.0163, 0.6772, 0.8532],\n",
       "           [0.6579, 0.1048, 0.8861, 0.4390, 0.3512],\n",
       "           [0.5400, 0.7521, 0.6172, 0.8874, 0.1668]],\n",
       " \n",
       "          [[0.8872, 0.2539, 0.8344, 0.2357, 0.7378],\n",
       "           [0.7767, 0.9697, 0.9080, 0.8605, 0.9434],\n",
       "           [0.2064, 0.8816, 0.7094, 0.2332, 0.6700],\n",
       "           [0.8967, 0.0991, 0.1438, 0.8884, 0.5572]]]]),\n",
       " tensor([[0.2074, 0.2054, 0.5633, 0.3730, 0.8804, 0.4153, 0.4657, 0.5863, 0.3955,\n",
       "          0.8333, 0.1516, 0.9421, 0.3776, 0.0644, 0.0509, 0.3256, 0.8615, 0.0672,\n",
       "          0.2774, 0.1863, 0.8762, 0.0150, 0.1385, 0.0170, 0.7206, 0.3973, 0.1492,\n",
       "          0.1111, 0.1172, 0.1541, 0.6786, 0.7248, 0.4294, 0.8560, 0.8313, 0.6508,\n",
       "          0.5372, 0.6621, 0.8178, 0.3615, 0.0988, 0.4579, 0.3510, 0.6998, 0.2864,\n",
       "          0.4671, 0.0352, 0.8265, 0.4562, 0.7443, 0.5871, 0.8109, 0.0035, 0.3449,\n",
       "          0.8018, 0.7328, 0.4198, 0.7526, 0.5755, 0.8206],\n",
       "         [0.9563, 0.4376, 0.7134, 0.9166, 0.5823, 0.9733, 0.5761, 0.2175, 0.8851,\n",
       "          0.2554, 0.0991, 0.2961, 0.9611, 0.6106, 0.0844, 0.1208, 0.1454, 0.3462,\n",
       "          0.2565, 0.8868, 0.0856, 0.0987, 0.3582, 0.1966, 0.4465, 0.8674, 0.2877,\n",
       "          0.4052, 0.2205, 0.4557, 0.1451, 0.9533, 0.5063, 0.4957, 0.2135, 0.6175,\n",
       "          0.5078, 0.8189, 0.5772, 0.3823, 0.4847, 0.6292, 0.5457, 0.1908, 0.3470,\n",
       "          0.6417, 0.7658, 0.8591, 0.5521, 0.3953, 0.6925, 0.5183, 0.6053, 0.6116,\n",
       "          0.0647, 0.4607, 0.1917, 0.6527, 0.4685, 0.8534],\n",
       "         [0.3328, 0.9210, 0.2726, 0.5785, 0.2683, 0.5808, 0.8809, 0.4370, 0.7120,\n",
       "          0.0318, 0.0265, 0.6075, 0.2960, 0.7722, 0.7348, 0.0707, 0.1790, 0.0715,\n",
       "          0.7037, 0.9577, 0.9155, 0.5568, 0.0687, 0.7477, 0.5852, 0.6463, 0.1502,\n",
       "          0.0163, 0.6772, 0.8532, 0.6579, 0.1048, 0.8861, 0.4390, 0.3512, 0.5400,\n",
       "          0.7521, 0.6172, 0.8874, 0.1668, 0.8872, 0.2539, 0.8344, 0.2357, 0.7378,\n",
       "          0.7767, 0.9697, 0.9080, 0.8605, 0.9434, 0.2064, 0.8816, 0.7094, 0.2332,\n",
       "          0.6700, 0.8967, 0.0991, 0.1438, 0.8884, 0.5572]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(3, 3, 4, 5)\n",
    "Y = X.view(X.shape[0], -1)\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
